{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63fdaca0-29a0-473e-a96b-841d71638994",
   "metadata": {},
   "source": [
    "# Errata for Lecture 13\n",
    "\n",
    "In the lecture on Oct 10, I presented the notion of an optimal classifier in the sense of minimizing\n",
    "risk when provided imperfect features.  Perfect features would have no region of ambiguity.\n",
    "Where there is ambiguity in the features, it means that the features of a given feature vector\n",
    "could occur with non-zero probability in samples belonging to more than one class.   \n",
    "\n",
    "Let $\\alpha_i$ denote action $i$.\n",
    "\n",
    "Let $\\lambda(\\alpha_i | \\omega_j) = \\lambda_{ij}$ the loss from taking action $\\alpha_i$ given that\n",
    "the true class is $\\omega_j$.  \n",
    "\n",
    "The risk $R$ for action $\\alpha_i$ given vector $\\bar{x}$ is the expected loss across all \n",
    "possible decisions (classifications) made by our classifier \n",
    "\n",
    "$$\n",
    "   R(\\alpha_i | \\bar{x}) = \\sum_{i=1}^K \\lambda_{ij} P(\\omega_j| \\bar{x}) \\hspace{1in} (1)\n",
    "$$\n",
    "\n",
    "$P(\\omega_j | \\bar{x})$ is the *posterior probability* of class $\\omega_j$ given a feature\n",
    "vector $\\bar{x}$.  We can rewrite the *posterior probability* in terms of its likelihood\n",
    "and prior using Bayes' Law:\n",
    "\n",
    "$$\n",
    "   P(\\omega_j | \\bar{x}) = \\frac{P(\\bar{x} | \\omega_j) P(\\omega_j)}{P(\\bar{x})} \\hspace{1in} (2)\n",
    "$$\n",
    "\n",
    "where $P(\\bar{x} | \\omega_j)$ is called the likelihood of $\\bar{x}$ across all instances of\n",
    "class $\\omega_j$ and $P(\\omega_j)$ is the prior probability of $\\omega_j$ meaning the probability\n",
    "of encountered instances of $\\omega_j$ without any conditions (i.e., if I were to randomly\n",
    "gather samples from the environment in which the classifier would be employed).\n",
    "\n",
    "Using Bayes' law, we can rewrite $(2)$ as \n",
    "\n",
    "$$\n",
    "   R(\\alpha_i | \\bar{x}) = \\frac{1}{P(\\bar{x})} \\sum_{i=1}^K \\lambda_{ij} P(\\bar{x} | \\omega_j) P(\\omega_j) \\hspace{1in} (3)\n",
    "$$\n",
    "\n",
    "A minimum risk classifier uses the decision rule that when given an input with \n",
    "an ambiguous feature vector $\\bar{x}$, it chooses the class that presents the minimum risk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104e7da-ff69-40c9-a7f8-57f6479379cb",
   "metadata": {},
   "source": [
    "I then restated this for the binary classification case.  The classifier $h$ decides\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    " \\omega_1 \\hspace{0.5in} & \n",
    "     \\text{if } \\frac{\\lambda_{11} P(\\bar{x} | \\omega_1) P(\\omega_1) + \\lambda_{12} P(\\bar{x} | \\omega_2) P(\\omega_2)}\n",
    "                    {P(\\bar{x})} \n",
    "     <\n",
    "       \\frac{\\lambda_{21} P(\\bar{x} | \\omega_1) P(\\omega_1) + \\lambda_{22} P(\\bar{x} | \\omega_2) P(\\omega_2)}\n",
    "            {P(\\bar{x})} \n",
    "       \\hspace{1in} (4)  \\\\\n",
    " \\omega_2  \\hspace{0.5in} & \\text{ otherwise}\n",
    "\\end{align}\n",
    "\n",
    "Because $P(\\bar{x})$ is the same on both sides, we can rewrite this\n",
    "\n",
    "\\begin{align}\n",
    " \\omega_1 \\hspace{0.5in} & \n",
    "     \\text{if } \\lambda_{11} P(\\bar{x} | \\omega_1) P(\\omega_1) + \\lambda_{12} P(\\bar{x} | \\omega_2) P(\\omega_2)  \n",
    "     <\n",
    "       \\lambda_{21} P(\\bar{x} | \\omega_1) P(\\omega_1) + \\lambda_{22} P(\\bar{x} | \\omega_2) P(\\omega_2)           \n",
    "       \\hspace{1in} (5)  \\\\\n",
    " \\omega_2  \\hspace{0.5in} & \\text{ otherwise}\n",
    "\\end{align}\n",
    "\n",
    "Grouping terms in (5) yields\n",
    "\n",
    "\\begin{align}\n",
    " \\omega_1 \\hspace{0.5in} & \n",
    "     \\text{if } (\\lambda_{11} - \\lambda_{21}) P(\\bar{x} | \\omega_1) P(\\omega_1)  \n",
    "     <\n",
    "       (\\lambda_{22} - \\lambda_{12}) P(\\bar{x} | \\omega_2) P(\\omega_2)           \n",
    "       \\hspace{1in} (6)  \\\\\n",
    " \\omega_2  \\hspace{0.5in} & \\text{ otherwise}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "<!-- \n",
    "Reasons for using log likelihoods as a loss function:\n",
    "\n",
    "1. **Numerical stability:** Likelihood values are often products of many probabilities especially in\n",
    "models involving multiple observations.  The products of small probabilities\n",
    "results in extermely smaller numbers. These small values can cause underflow\n",
    "in floating-point arithmetic, making the calculations unreliable. By taking\n",
    "the logarithm of the likelihood, we convert these products into sums, which\n",
    "are more numerically stable and cheaper to compute.\n",
    "\n",
    "2. **Simplified gradient computation:** When we take a log or products, multiplications\n",
    "turn into additions.  When compute a derivative we can compute it separately\n",
    "for each summand, simplifyibng the gradient calculation of the loss function.\n",
    "This makes gradient descent easier and more efficient to compute.\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbae9a2-0b99-49f4-911c-7f0c3b514030",
   "metadata": {},
   "source": [
    "We can present this as a ratio\n",
    "\n",
    "\\begin{align}\n",
    " \\omega_1 \\hspace{0.5in} & \n",
    "     \\text{if } \\frac{P(\\bar{x} | \\omega_1)}{P(\\bar{x} | \\omega_2)}\n",
    "       < \\frac{\\lambda_{22} - \\lambda_{12}}{\\lambda_{11} - \\lambda_{21}} \\frac{P(\\omega_2)}{P(\\omega_1)}\n",
    "       \\hspace{1in} (7)  \\\\\n",
    " \\omega_2  \\hspace{0.5in} & \\text{ otherwise}\n",
    "\\end{align}\n",
    "\n",
    "I should have left it there, but I added this next erroneous inequality.\n",
    "\n",
    "\\begin{align}\n",
    " \\omega_1 \\hspace{0.5in} & \n",
    "     \\text{if } \\frac{P(\\bar{x} | \\omega_1)}{P(\\bar{x} | \\omega_2)}\n",
    "       > \\frac{\\lambda_{12}- \\lambda_{22}}{\\lambda_{21} - \\lambda_{11}} \\frac{P(\\omega_2)}{P(\\omega_1)}\n",
    "       \\hspace{1in} (8)  \\\\\n",
    " \\omega_2  \\hspace{0.5in} & \\text{ otherwise}\n",
    "\\end{align}\n",
    "\n",
    "I should not have flipped the less than symbol to a greater than symbol.  It would've been better\n",
    "to just stop at (7).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
