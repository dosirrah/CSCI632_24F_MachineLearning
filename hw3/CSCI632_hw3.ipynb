{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff83ffe-c7d1-408f-9365-583348ca4741",
   "metadata": {},
   "source": [
    "# CSCI 632 Machine Learning Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f846fff-ebaf-42e2-a19b-877f8e262e63",
   "metadata": {},
   "source": [
    "**Clarification**\n",
    "\n",
    "In Problem 1(a), I changed what was requested \"Find the risk function for the case where the system chooses to classify the observation (i.e., no rejection), $R(\\alpha | \\mathbf{x}, \\text{choose})$\" to \"Find the risk function for the case where the system chooses to classify the observation (i.e., no rejection), $R(\\alpha_i | \\mathbf{x}, \\text{choose})$\" .  I believe this is clearer.\n",
    "\n",
    "**Correction**\n",
    "\n",
    "In Problem 1(a), I changed\n",
    "\n",
    "$$R(\\alpha_i|\\mathbf{x}, \\text{choose}) < R(\\alpha_i|\\mathbf{x}, \\text{reject})$$\n",
    "\n",
    "to\n",
    "\n",
    "$$R(\\alpha_i|\\mathbf{x}, \\text{choose}) \\leq R(\\alpha_i|\\mathbf{x}, \\text{reject})$$.\n",
    "\n",
    "Both rules will minimize risk, but this change causes the system to choose the class with label $\\omega_i$ when the risk is equal either way, which\n",
    "corresponds to \n",
    "\n",
    "$$P(\\omega_i | \\mathbf{x}) \\geq 1 - \\frac{\\lambda_r}{\\lambda_s}$$\n",
    "\n",
    "The problem has been changed to reflect this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a9affa-bce9-43ff-9c8e-bcbc6c4c644a",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "* **Insert all code, plots, results, and discussion** into this Jupyter Notebook.\n",
    "* Your homework should be submitted as a **single Jupyter Notebook** (.ipynb file).\n",
    "* While working, you use Google Colab by uploading this notebook and performing work there. Once complete, export the notebook as a Jupyter Notebook (.ipynb) and submit it to **Blackboard.**\n",
    "\n",
    "You can answer mathematical questions either by:\n",
    "* using LaTeX in a markdown cell, or\n",
    "* pasting a scanned or photographed handwritten answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea41b8-3c24-46a7-ba6a-47e7e3cca410",
   "metadata": {},
   "source": [
    "**Problem 1** (20 pts) Consider the problem of Automatic \n",
    "Content Recognition (ACR).  ACR aims to recognize content such as TV\n",
    "shows, movies, or sports. In this scenario, we treat each piece of\n",
    "content as a class in a multiclass classification problem, where there\n",
    "are $K$ known pieces of content. The content may be re-encoded multiple \n",
    "times during distribution to adapt to available bandwidth or loss\n",
    "characteristics, and may be subject to \"visual enhancements” by devices\n",
    "like set-top boxes or TVs, all of which introduce noise. Despite these\n",
    "distortions, the model must classify the content.\n",
    "\n",
    "Because video and audio are composed of a steady stream of frames or audio \n",
    "samples from which we generate feature vectors $\\textbf{x}(t)$ at points in time\n",
    "$t$, we are not forced to immediately make a decision.  Instead, the ACR system can\n",
    "choose to *reject* an observation as unrecognizable instead of\n",
    "misclassifying it. When the cost of rejection is low, it may be\n",
    "beneficial to reject an observation and wait for more data to\n",
    "improve classification accuracy.\n",
    "\n",
    "Let $\\lambda$ denote our loss function.\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda(\\alpha_i| \\omega_j) =\n",
    "\\begin{cases}\n",
    "  0 & i = j \\text{ for } i, j = 1, ..., K \\\\\n",
    "  \\lambda_r & i = K + 1 \\quad (\\text{rejection}) \\\\\n",
    "  \\lambda_s & \\text{if } i \\neq j \\quad (\\text{substitution}) \n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "* $\\lambda_r$ is the loss incurred for rejecting the observation\n",
    "  (i.e., deciding to wait).  Let action $(K+1)$ denote rejection.\n",
    "* $\\lambda_s$ is the loss incurred for a substitution error (i.e.,\n",
    "  classifying the observation as the wrong content).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1332e1-e7e4-42e6-ac0a-b9e80f7da886",
   "metadata": {},
   "source": [
    "**(a)** At decision time, the ACR system must either choose to classify the content into one of the known classes or reject the observation (i.e., postpone the decision). Find the risk function for the case where the system chooses to classify the observation (i.e., no rejection), $R(\\alpha_i | \\mathbf{x}, \\text{choose})$. The risk function represents the expected loss conditioned on the observation $\\mathbf{x}$ and the decision to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbc373-c92d-4184-b960-11c2566bdf80",
   "metadata": {},
   "source": [
    "**(b)** If the risk of classifying the content (i.e., choosing) exceeds the\n",
    "risk of rejecting the observation, then choosing will increase the overall\n",
    "risk. To minimize the risk, we should only choose to classify if:\n",
    "\n",
    "$$R(\\alpha_i|\\mathbf{x}, \\text{choose}) \\leq R(\\alpha_i|\\mathbf{x}, \\text{reject})$$\n",
    "\n",
    "Using this criterion, show that the minimum risk is obtained by classifying\n",
    "the observation as class $\\omega_i$ if:\n",
    "\n",
    "$$P(\\omega_i|\\mathbf{x}) \\geq P(\\omega_j|\\mathbf{x}) \\text{ for all } j \\neq i$$\n",
    "which means choosing the class wiht the highest posterior probability.\n",
    "Furthermore, show that we should classify the observation as $\\omega_i$ if\n",
    "\n",
    "$$P(\\omega_i | \\mathbf{x}) \\geq 1 - \\frac{\\lambda_r}{\\lambda_s}, $$\n",
    "\n",
    "and reject otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2718973d-c8a3-42ef-9303-a2f9f3964285",
   "metadata": {},
   "source": [
    "**(c)** What happens to the decisions made when $\\lambda_r = 0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1dd0cb-01d9-4099-a575-692a3b33383d",
   "metadata": {},
   "source": [
    "**(d)** What happens to the decisions made when $\\lambda_r > \\lambda_s$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ccff0-7b29-44d5-aa89-a33ab5809c0a",
   "metadata": {},
   "source": [
    "**Problem 2**\n",
    "\n",
    "Given a source that outputs a sequence of zeros and ones.  Each zero or one is called a symbol.  \n",
    "In this problem, each symbol is independent of the others.  The probability of a 1 is given by\n",
    "$p$ and thus the probability of a zero is $1-p$.\n",
    "\n",
    "(a) Compute the Shannon entropy for $p=0.3$\n",
    "\n",
    "(b) Plot the Shannon entropy in bits for many values of $p$ in the interval $[0,1]$.\n",
    "\n",
    "(c) Consecutive occurrences of the same symbol are called runs. `000001` is a run\n",
    "of 5 zeros followed by a 1.  *Run length encoding* \n",
    "is sometimes used when the input data is dominated by long runs of the same symbol.\n",
    "`AAABBBBBEEEE` would be encoded as `3A5B4E`. \n",
    "\n",
    "*Zero run length encoding* is a variant typically used with binary sequences\n",
    "in which the dominant symbol is zero.  We allocate a fixed length field $k$\n",
    "bits wide to denote the run length of consecutive zeros before a 1.  If all bits\n",
    "in the field are ones, this is a reserved value indicating an *incomplete run*.\n",
    "\n",
    "An *incomplete run* occurs when a run of zeros reaches the maximum length\n",
    "representable by the $k$-bit field, which is $2^k-1$, and continues with\n",
    "additional zeros without an intervening 1.  In this case, the next field \n",
    "encodes the length of the continued run.\n",
    "\n",
    "For example, if $k=4$, the zero run length encoding creates the given\n",
    "outputs for the input sequences shown in Table 1.\n",
    "\n",
    "$$\\text{Table 1}$$\n",
    "\n",
    "| input            |  output      | compression$\\dagger$ |  explanation                            |\n",
    "|------------------|--------------|-------------|--------------------------------------------------|\n",
    "| 1                | 0000         | 1/4         | zero zeroes then 1                               |\n",
    "| 01               | 0001         | 1/2         | one zero then 1                                  |\n",
    "| 11               | 00000000     | 1/4         | 0 zeroes then 1, 0 zeros then 1                  |\n",
    "| 0000000001       | 1001         | 9/4=2.25    | $1001_2=9$, 9 zeroes then 1.                     |\n",
    "| 0010001          | 00100011     | 7/8         | $0010_2=2$ zeroes, $0011_2=3$ zeroes.            |\n",
    "| 32 zeroes then 1 | 111111110010 | 33/12=2.75  | $15(1111_2) + 15(1111_2) + 2 = 32$ zeroes then 1 |\n",
    "\n",
    "\n",
    "$$\\dagger \\text{ compression ratio is } \\frac{\\text{uncompressed size}}{\\text{compressed size}}$$\n",
    "\n",
    "\n",
    "In the last row we reduce 33 input symbols to 12 output symbols.  This has a compression\n",
    "ratio of $33/12 = 2.75$\n",
    "\n",
    "For $p=0.003$, (i.e., the probability of a 1 is 0.3%), implement zero run length encoding.  \n",
    "\n",
    "I uploaded the file `p2c.txt.gz`.  Decompress this file using `gunzip` before using it.\n",
    "\n",
    "Use your compression algorithm to compress the data file `p2c.txt` that uses ASCII 0\n",
    "and 1 to denote zeros and ones.  Output the compressed text file with name `p2c_k4.txt`.\n",
    "The output file should use ASCII zeros and ones, but compress the data using zero run\n",
    "length encoding with k=4.\n",
    "\n",
    "(d) Plot the compression ratio of `p2c.txt` using the zero run length encoder \n",
    "built in (c) as a function of $k$ for $k \\in (4, 16)$.  Put $k$ on the x-axis and\n",
    "compression ratio on the y-axis.\n",
    "\n",
    "(e) Plot the length of the output file as $k$ varies from 4 to 16. Plot a horizontal\n",
    "line at $n \\times H(x)$, where $n$ is the length of the input file `p2c.txt` and $H(x)$ is\n",
    "the Shannon entropy computed in (b).  Plot a second horizontal line for the length of the\n",
    "file `p2c.txt.gz`. The value $n \\times H(X)$ represents the theoretical lower bound \n",
    "achievable for lossless compression.  NOTE: To make this fair, consider the length in \n",
    "units of symbols.  Since your symbols are ascii characters, multiply the length of \n",
    "the gzip file by 8.\n",
    "\n",
    "(f) How does $n \\times H(X)$ from (b) compare to the length of the compressed files\n",
    "using your zero run-length encoding vs. gzip?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70282a5d-9702-4a9b-9417-30b11a13b2cf",
   "metadata": {},
   "source": [
    "**Problem 3**\n",
    "\n",
    "In classification problems, our classifier estimates the posterior \n",
    "probability $P(\\omega_i | \\bar{x})$ for every class.\n",
    "This can be expressed as a vector of probabilities.\n",
    "For example, if we are in the region of ambiguity for three different classes\n",
    "such that $P(\\omega_1 | \\bar{x}) = 0.3$, $P(\\omega_2 | \\bar{x}) = 0.5$\n",
    "and $P(\\omega_3 | \\bar{x}) = 0.2$.  The classifier doesn't know the \n",
    "true posterior probabilities so it estimates them.  \n",
    "\n",
    "Let $\\hat{P}(\\omega_j | \\bar{x})$ denote the estimated posterior probability\n",
    "of class $j$ given feature vector $\\bar{x}$.  The estimated posterior\n",
    "probability is called the *predicted probability*.\n",
    "\n",
    "Let $y$ denote the true class.  When discussing training sets, in\n",
    "previous lectures we introduced $y^{(i)}$ as a scalar denoting the \n",
    "true class for the $i$th observation $(\\mathbf{x}^{(i)}, y^{(i)})$.\n",
    "We continue to use $y^{(i)}$ notation in the context of training sets.\n",
    "\n",
    "Let $\\hat{y}_j$ denotes the predicted probability for class $j$.\n",
    "$\\hat{y}_j$ estimates the posterior probability. Meaning\n",
    "\n",
    "$$\\hat{y}_j = \\hat{P}(\\omega_j | \\bar{x}) \\hspace{2in} (3.1)$$\n",
    "\n",
    "Let $\\mathbf{\\hat{y}}$ denote the column vector of the predicted \n",
    "probabilities.\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = \\begin{bmatrix}\n",
    "\\hat{P}(y = 1 | \\mathbf{x}) \\\\\n",
    "\\hat{P}(y = 2 | \\mathbf{x}) \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{P}(y = K | \\mathbf{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "An accurate classifier would closely model the posterior\n",
    "probabilities such that\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}} \\approx \\begin{bmatrix}\n",
    "P(y = 1 | \\mathbf{x}) \\\\\n",
    "P(y = 2 | \\mathbf{x}) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = K | \\mathbf{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus for the example probabilities $P(\\omega_1 | \\bar{x}) = 0.3$, $P(\\omega_2 | \\bar{x}) = 0.5$\n",
    "and $P(\\omega_3 | \\bar{x}) = 0.2$, $\\mathbf{\\hat{y}}$ becomes\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}} \\approx \\begin{bmatrix}\n",
    "0.3 \\\\\n",
    "0.2 \\\\\n",
    "0.5\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717cb7fc-1191-4b18-82fb-2d364933a41e",
   "metadata": {},
   "source": [
    "Cross-entropy is a widely used loss function in\n",
    "machine learning, especially for multi-class classification tasks, \n",
    "because it measures a notion of distance between the predicted probability\n",
    "distribution and the true distribution.\n",
    "\n",
    "The cross-entropy between two probability distributions $P$ and $Q$\n",
    "over the same set of events is defined as:\n",
    "\n",
    "$$H(P, Q) = - \\sum_{j} P(j) \\log Q(j)$$\n",
    "\n",
    "For classification we replace $P$ with a distribution that places\n",
    "100% of the probabilty on the correct class, and we replace\n",
    "$Q$ with $\\mathbf{\\hat{y}}$.\n",
    "\n",
    "In other words\n",
    "$P(y=k) = 1$ where $k$ is the true class and for all $j\\neq k$, \n",
    "$P(y = j) = 0$.  This probability assignment can be represented using\n",
    "an indicator function.\n",
    "\n",
    "An indicator function is defined by\n",
    "\n",
    "$$\n",
    "\\mathbb{1}\\{ y = k \\} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } y = k \\\\\n",
    "0 & \\text{if } y \\neq k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Using an indicator function, cross entropy becomes\n",
    "\n",
    "$$H(\\mathbb{1}\\{ y = k \\}, \\mathbf{\\hat{y}}) = - \\sum_{j=1}^K \\mathbb{1}\\{ y = k \\} \\log \\mathbf{\\hat{y}}_j \\hspace{2in} (3.2)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7b2de-14e2-4b85-a6e5-e76246675296",
   "metadata": {},
   "source": [
    "\n",
    "Indicator functions are commonly used, but I prefer a less heavy notation using\n",
    "what is called a *one-hot vector*.  As with indicator functions, one-hot vectors\n",
    "are also heavily used in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435c7bf-675a-4fda-87df-d034a1d9e57f",
   "metadata": {},
   "source": [
    "Let $\\mathbf{y}(k)$ denote the one-hot vector wherein $y=k$ corresponds to the \n",
    "$k$th element being 1, e.g.,\n",
    "\n",
    "$$\n",
    "\\mathbf{y}(k) = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where the $1$ above appears in the $k$th row of the column vector.\n",
    "\n",
    "For brevity, we may simply refer to the one-hot vector as $\\mathbf{y}$.  Note that $y$ is a \n",
    "scalar denoting the true class, and $\\mathbf{y}$ is the one-hot vector with 100% of the\n",
    "probability at the index of the true class.\n",
    "\n",
    "Using matrix notation, (3.2) can be restated as \n",
    "\n",
    "$$H(\\mathbf{y}, \\mathbf{\\hat{y}}) = - \\mathbf{y}^T \\log \\mathbf{\\hat{y}} \\hspace{2in} (3.3)$$\n",
    "\n",
    "In classification problems, the loss function is often denoted by a calligraphic L, i.e.,\n",
    "$\\mathcal{L}$, thus when we use cross-entropy $H$ as a loss function, we rewrite (3.3) as\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{y}, \\mathbf{\\hat{y}}) = - \\mathbf{y}^T \\log \\mathbf{\\hat{y}} \\hspace{2in} (3.4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fbb86f-a1d6-4682-ad4a-b39a5ff5801d",
   "metadata": {},
   "source": [
    "**(a)** Suppose we have a four-class classification problem \n",
    "$K = 4$, and for a particular data point, the true label is $y=3$. Write the one-hot vector $\\mathbf{y}$ for y=3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc08c16-c743-49b2-89b7-525523a939f9",
   "metadata": {},
   "source": [
    "**(b)** The predicted probability vector $\\mathbf{\\hat{y}}$ for this data point\n",
    "output from our model is:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = \\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2 \\\\\n",
    "0.6 \\\\\n",
    "0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute the cross-entropy loss $\\mathcal{L}$ when y = 3.\n",
    "\n",
    "**(c)** If the true class were $y=1$, what would $\\mathcal{L}$ equal?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb486c-0dc9-4634-a69e-b9cbcc529503",
   "metadata": {},
   "source": [
    "**(d)** If the predicted probability vector is itself a one-hot vector where $\\hat{y}_k = 1$ and the true class is $y=k$, what is the loss $\\mathcal{L}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827db772-0b9d-411b-98dc-98b172e29457",
   "metadata": {},
   "source": [
    "**(e)** The classifier is certain that $\\mathbf{x}$ corresponds to class 4, i.e., $\\hat{y}_4 = 1$, but the true class is $y=3$.  What happens to $\\mathcal{L}$.  Aside: this problem can be avoided by clipping the predicted probabilities such that \n",
    "$\\hat{y}_k \\in [\\epsilon, 1 - \\epsilon]$ where $\\epsilon$  is a small value like $10^{-8}$, to avoid $\\log(0)$ and $\\log(1)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26327ed-4025-41da-8787-ed1d587731b4",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
